{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ef502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1. IMPORTS ----\n",
    "import os, re, json, requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from time import sleep\n",
    "\n",
    "# Load .env file and NLTK resources\n",
    "load_dotenv()\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# ---- 2. API KEYS ----\n",
    "NEWSAPI_KEY = os.getenv(\"NEWSAPI_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "SLACK_WEBHOOK = os.getenv(\"SLACK_WEBHOOK\")\n",
    "\n",
    "# ---- 3. FETCH NEWS ----\n",
    "def fetch_news():\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    params = {\n",
    "        \"q\": \"Artificial Intelligence\",\n",
    "        \"pageSize\": 100,\n",
    "        \"language\": \"en\",\n",
    "        \"sortBy\": \"publishedAt\",\n",
    "        \"apiKey\": NEWSAPI_KEY\n",
    "    }\n",
    "\n",
    "    r = requests.get(url, params=params)\n",
    "    try:\n",
    "        data = r.json()\n",
    "    except Exception:\n",
    "        print(\"âŒ Invalid JSON response from NewsAPI\")\n",
    "        print(\"Response text:\", r.text)\n",
    "        return []\n",
    "\n",
    "    if data.get(\"status\") != \"ok\":\n",
    "        print(\"âŒ NewsAPI error:\", data.get(\"message\", \"Unknown error\"))\n",
    "        return []\n",
    "\n",
    "    return data.get(\"articles\", [])\n",
    "\n",
    "# ---- 4. TEXT CLEANING ----\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = text.lower()\n",
    "    return \" \".join([w for w in text.split() if w not in STOPWORDS])\n",
    "\n",
    "# ---- 5. GEMINI SENTIMENT (BATCH) ----\n",
    "def get_sentiment_data_in_batch(headlines: list, batch_size=5) -> list:\n",
    "    all_results = []\n",
    "\n",
    "    for i in range(0, len(headlines), batch_size):\n",
    "        batch = headlines[i:i + batch_size]\n",
    "        numbered = \"\\n\".join([f\"{j+1}. {h}\" for j, h in enumerate(batch)])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are a sentiment analysis expert.\n",
    "Analyze each of the following news headlines independently.\n",
    "\n",
    "Return a JSON array with each element like:\n",
    "[{{\"label\": \"Positive\", \"score\": 0.85}}, ... ]\n",
    "\n",
    "Headlines:\n",
    "{numbered}\n",
    "\"\"\"\n",
    "\n",
    "        url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-lite:generateContent?key={GEMINI_API_KEY}\"\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        payload = {\n",
    "            \"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}],\n",
    "            \"generationConfig\": {\"temperature\": 0.1, \"maxOutputTokens\": 512}\n",
    "        }\n",
    "\n",
    "        # Retry logic\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                r = requests.post(url, headers=headers, json=payload, timeout=90)\n",
    "                r.raise_for_status()\n",
    "                data = r.json()\n",
    "                text_out = data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "                text_out = re.sub(r\"```json|```\", \"\", text_out).strip()\n",
    "                parsed = json.loads(re.search(r\"\\[.*\\]\", text_out, re.DOTALL).group(0))\n",
    "\n",
    "                for item in parsed:\n",
    "                    label = item.get(\"label\", \"Neutral\")\n",
    "                    score = float(item.get(\"score\", 0.0))\n",
    "                    all_results.append((label, score))\n",
    "\n",
    "                break  # success â†’ stop retrying\n",
    "            except Exception as e:\n",
    "                if attempt < 2:\n",
    "                    print(f\"âš  Timeout/error in batch {i//batch_size + 1}, retrying... ({attempt+1}/3)\")\n",
    "                    sleep(5)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"âŒ Error in batch {i//batch_size + 1}: {e}\")\n",
    "                    all_results.extend([(\"Neutral\", 0.0)] * len(batch))\n",
    "\n",
    "        sleep(1)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# ---- 6. SLACK ALERT (with emojis + signed score) ----\n",
    "def send_to_slack(title, url, sentiment, score):\n",
    "    if not SLACK_WEBHOOK:\n",
    "        return\n",
    "\n",
    "    emoji_map = {\n",
    "        \"Positive\": \"âœ…\",\n",
    "        \"Neutral\": \"âšª\",\n",
    "        \"Negative\": \"âŒ\"\n",
    "    }\n",
    "    emoji = emoji_map.get(sentiment, \"\")\n",
    "\n",
    "    # Show + / - sign for score\n",
    "    if sentiment == \"Positive\":\n",
    "        score_text = f\"+{score}\"\n",
    "    elif sentiment == \"Negative\":\n",
    "        score_text = f\"{score}\"  # already negative\n",
    "    else:\n",
    "        score_text = \"0\"\n",
    "\n",
    "    msg = f\"{emoji} {sentiment.upper()} NEWS ALERT (Score: {score_text})\\nâ€¢ Title: {title}\\nâ€¢ Link: {url}\"\n",
    "    requests.post(SLACK_WEBHOOK, json={\"text\": msg})\n",
    "\n",
    "# ---- 7. VISUALIZATIONS ----\n",
    "def visualize(df):\n",
    "    # --- Sentiment distribution ---\n",
    "    plt.figure()\n",
    "    df[\"sentiment_label\"].value_counts().plot(kind=\"bar\", color=[\"green\",\"red\",\"gray\"])\n",
    "    plt.title(\"Sentiment Distribution\")\n",
    "    plt.xlabel(\"Sentiment\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.savefig(\"sentiment_distribution.png\")\n",
    "\n",
    "    # --- Wordcloud ---\n",
    "    text = \" \".join(df[\"cleaned_content\"])\n",
    "    wc = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(\"wordcloud.png\")\n",
    "\n",
    "# ---- 8. MAIN PIPELINE ----\n",
    "def run_pipeline():\n",
    "    print(\"ðŸ” Fetching latest AI news...\")\n",
    "    articles = fetch_news()\n",
    "    if not articles:\n",
    "        print(\"No news fetched.\")\n",
    "        return\n",
    "\n",
    "    print(f\"ðŸ“¦ {len(articles)} articles fetched.\")\n",
    "    print(\"âš™ï¸ Running sentiment analysis using Gemini 2.5 Flash Lite...\")\n",
    "\n",
    "    headlines = [f\"{a.get('title','')} - {a.get('description','')}\" for a in articles]\n",
    "    sentiment_results = get_sentiment_data_in_batch(headlines, batch_size=5)\n",
    "\n",
    "    results = []\n",
    "    for i, a in enumerate(articles):\n",
    "        title = a.get(\"title\", \"\")\n",
    "        desc = a.get(\"description\", \"\")\n",
    "        content = a.get(\"content\", \"\")\n",
    "        full_text = f\"{title} {desc} {content}\"\n",
    "        cleaned = clean_text(full_text)\n",
    "\n",
    "        label, score = sentiment_results[i]\n",
    "        label_short = \"Positive\" if label.lower() == \"positive\" else \"Negative\" if label.lower() == \"negative\" else \"Neutral\"\n",
    "\n",
    "        # Make score signed\n",
    "        if label_short == \"Negative\":\n",
    "            score = -abs(score)\n",
    "        elif label_short == \"Positive\":\n",
    "            score = abs(score)\n",
    "        else:\n",
    "            score = 0.0\n",
    "\n",
    "        results.append({\n",
    "            \"src\": a.get(\"source\", {}).get(\"name\"),\n",
    "            \"title\": title,\n",
    "            \"description\": desc,\n",
    "            \"url\": a.get(\"url\"),\n",
    "            \"publishedAt\": a.get(\"publishedAt\"),\n",
    "            \"content\": content,\n",
    "            \"cleaned_content\": cleaned,\n",
    "            \"sentiment_score\": score,  # signed score\n",
    "            \"sentiment_label\": label_short\n",
    "        })\n",
    "\n",
    "        send_to_slack(title, a.get(\"url\"), label_short, score)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"ai_news_sentiment.csv\", index=False)\n",
    "    print(\"âœ… Sentiment analysis complete.\")\n",
    "    print(\"ðŸ“Š Generating visualizations...\")\n",
    "    visualize(df)\n",
    "    print(\"ðŸ–¼ï¸ Visualizations saved: sentiment_distribution.png, wordcloud.png\")\n",
    "\n",
    "# ---- 9. RUN ----\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
